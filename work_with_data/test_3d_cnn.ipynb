{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   #if like me you do not have a lot of memory in your GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #then these two lines force keras to use your CPU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import h5py\n",
    "\n",
    "import matplotlib as mpl\n",
    "COLOR = 'white'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_dataset):\n",
    "    data = {}\n",
    "    time_init =     \"Time:  0.00000E+00 y\"\n",
    "    time_final =    \"Time:  5.00000E+00 y\"\n",
    "    number_datapoints = 0\n",
    "    names_of_runs = []\n",
    "\n",
    "    for file in tqdm(list(os.listdir(path_dataset))):\n",
    "        try:\n",
    "            path_data = os.path.join(path_dataset,file)\n",
    "            if not os.path.isdir(path_data):\n",
    "                continue\n",
    "            print(file)\n",
    "            names_of_runs.append(file)\n",
    "            filename = os.path.join(path_data,\"pflotran.h5\") \n",
    "            with h5py.File(filename, \"r\") as f:\n",
    "                for key, value in f[time_final].items():\n",
    "                    if not key in data: # and not key=='Material_ID':\n",
    "                        data[key] = []\n",
    "                    #if not key=='Material_ID':\n",
    "                    if key=='Liquid_Pressure [Pa]':\n",
    "                        data[key].append(np.array(f[time_init][\"Liquid_Pressure [Pa]\"]))\n",
    "                    else:\n",
    "                        data[key].append(np.array(value))\n",
    "            number_datapoints += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"lololololololoo: {e}\")\n",
    "\n",
    "    for key,value in data.items():\n",
    "        data[key] = np.array(value)\n",
    "\n",
    "    return data, number_datapoints, names_of_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_df(path_dataset):\n",
    "    time_init =     \"Time:  0.00000E+00 y\"\n",
    "    time_final =    \"Time:  5.00000E+00 y\"\n",
    "    names_of_runs = []\n",
    "    names_of_properties = []\n",
    "    data = []\n",
    "\n",
    "    for file in tqdm(list(os.listdir(path_dataset))):\n",
    "        try:\n",
    "            path_data = os.path.join(path_dataset,file)\n",
    "            if not os.path.isdir(path_data):\n",
    "                continue\n",
    "            #print(file)\n",
    "            names_of_runs.append(file)\n",
    "            filename = os.path.join(path_data,\"pflotran.h5\") \n",
    "            interim_array = []\n",
    "            with h5py.File(filename, \"r\") as f:\n",
    "                for key, value in f[time_final].items():\n",
    "                    if key=='Liquid_Pressure [Pa]':\n",
    "                        interim_array.append(np.array(f[time_init][\"Liquid_Pressure [Pa]\"]))\n",
    "                    else:\n",
    "                        interim_array.append(np.array(value))\n",
    "                names_of_properties = list(f[time_final].keys())\n",
    "            data.append(interim_array)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"lololololololoo: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data=data, index=names_of_runs, columns=names_of_properties)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_colorbar(*args,**kwargs):\n",
    "    cax = make_axes_locatable(plt.gca()).append_axes(\"right\",size= 0.3,pad= 0.05)\n",
    "    plt.colorbar(*args,cax=cax,**kwargs)\n",
    "    \n",
    "def plot_sample(data, sample_id, view=\"top\"):\n",
    "    n_dims = len(data)\n",
    "    fig, axes = plt.subplots(n_dims,1,sharex=True,figsize=(20,3*n_dims))\n",
    "    plt.figure(figsize= (20,3*n_dims))\n",
    "    for i,(key,value) in zip(range(n_dims),data.items()):\n",
    "        plt.sca(axes[i])\n",
    "        field = value[sample_id]\n",
    "        if len(field.shape) != 3:\n",
    "            # no 3D data\n",
    "            continue\n",
    "        index = key.find(' [')\n",
    "        title = key\n",
    "        if index != -1:\n",
    "            title = key[:index]\n",
    "        plt.title(title)\n",
    "        if view==\"topish\":\n",
    "            plt.imshow(field[:,:,-3])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        elif view==\"side\":\n",
    "            plt.imshow(field[11,:,::-1].T)\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"z\")\n",
    "        elif view==\"side_hp\":\n",
    "            plt.imshow(field[8,:,::-1].T)\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"z\")\n",
    "        elif view==\"top_hp\":\n",
    "            plt.imshow(field[:,:,8])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        elif view==\"top\":\n",
    "            plt.imshow(field[:,:,-1])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        aligned_colorbar(label=key)\n",
    "    plt.show()\n",
    "\n",
    "def plot_sample_df(df, run_id, view=\"top\"):\n",
    "    n_dims = len(df.columns)\n",
    "    fig, axes = plt.subplots(n_dims,1,sharex=True,figsize=(20,3*n_dims))\n",
    "    plt.figure(figsize= (20,3*n_dims))\n",
    "    for column, (i) in zip(df.columns, range(n_dims)):\n",
    "        plt.sca(axes[i])\n",
    "        field = df.at[run_id, column]\n",
    "        if len(field.shape) != 3:\n",
    "            # no 3D data\n",
    "            continue\n",
    "        index = column.find(' [')\n",
    "        title = column\n",
    "        if index != -1:\n",
    "            title = column[:index]\n",
    "        plt.title(title)\n",
    "        if view==\"topish\":\n",
    "            plt.imshow(field[:,:,-3])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        elif view==\"side\":\n",
    "            plt.imshow(field[11,:,::-1].T)\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"z\")\n",
    "        elif view==\"side_hp\":\n",
    "            plt.imshow(field[8,:,::-1].T)\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"z\")\n",
    "        elif view==\"top_hp\":\n",
    "            plt.imshow(field[:,:,8])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        elif view==\"top\":\n",
    "            plt.imshow(field[:,:,-1])\n",
    "            plt.xlabel(\"y\")\n",
    "            plt.ylabel(\"x\")\n",
    "        aligned_colorbar(label=column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 169.43it/s]\n"
     ]
    }
   ],
   "source": [
    "path_dir = \"/home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified\"\n",
    "dataset_name = \"dataset_HDF5_test\" #dataset_HDF5_uniformly_distributed_data\n",
    "path_dataset = os.path.join(path_dir, dataset_name)\n",
    "\n",
    "#data, number_datapoints, names_datapoints = read_data(path_dataset)\n",
    "#print(data[\"Liquid_Pressure [Pa]\"][0,:,:,:].shape)\n",
    "#plot_sample(data,2,view=\"topish\")\n",
    "#print(number_datapoints)\n",
    "\n",
    "df = read_data_df(path_dataset)\n",
    "#plot_sample_df(df,\"RUN_4\", \"top_hp\")\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find maximum alias hp\n",
    "# print(np.where(data[\"Material_ID\"]==np.max(data[\"Material_ID\"])))\n",
    "\n",
    "## data cleaning: cut of edges - to get rid of problems with boundary conditions\n",
    "def data_cleaning(data):\n",
    "    for key, (value) in data.items():\n",
    "        data[key] = data[key][:,1:-1,1:-3,1:-1]\n",
    "        print(f\"{key} :{np.shape(data[key])}\")\n",
    "    return data\n",
    "\n",
    "def data_cleaning_df(df):\n",
    "    for label, content in df.items():\n",
    "        for index in range(len(content)):\n",
    "            content[index] = content[index][1:-1,1:-3,1:-1]\n",
    "    return df\n",
    "\n",
    "df = data_cleaning_df(df)\n",
    "#plot_sample(df,\"RUN_1\",view=\"side_hp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle data along \"RUN_\"-axis\n",
    "# is already shuffled from reading-order but whatever\n",
    "df_shuffled = df.sample(frac=1)\n",
    "\n",
    "## split data into test and training data\n",
    "train_size_percent = 0.8\n",
    "train_size_abs = int(train_size_percent * df.shape[0]) # cuts instead of rounds\n",
    "\n",
    "df_train = df_shuffled[:train_size_abs]\n",
    "df_test = df_shuffled[train_size_abs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0\n",
      "RUN_2 Liquid X-Velocity [m_per_y]\n",
      "RUN_4 Liquid X-Velocity [m_per_y]\n",
      "RUN_1 Liquid X-Velocity [m_per_y]\n",
      "RUN_3 Liquid X-Velocity [m_per_y]\n",
      "1.0 -1.0\n",
      "RUN_2 Liquid Y-Velocity [m_per_y]\n",
      "RUN_4 Liquid Y-Velocity [m_per_y]\n",
      "RUN_1 Liquid Y-Velocity [m_per_y]\n",
      "RUN_3 Liquid Y-Velocity [m_per_y]\n",
      "1.0 -1.0\n",
      "RUN_2 Liquid Z-Velocity [m_per_y]\n",
      "RUN_4 Liquid Z-Velocity [m_per_y]\n",
      "RUN_1 Liquid Z-Velocity [m_per_y]\n",
      "RUN_3 Liquid Z-Velocity [m_per_y]\n",
      "1.0 -1.0\n",
      "RUN_2 Liquid_Pressure [Pa]\n",
      "RUN_4 Liquid_Pressure [Pa]\n",
      "RUN_1 Liquid_Pressure [Pa]\n",
      "RUN_3 Liquid_Pressure [Pa]\n",
      "1.0 -1.0\n",
      "RUN_2 Material_ID\n",
      "RUN_4 Material_ID\n",
      "RUN_1 Material_ID\n",
      "RUN_3 Material_ID\n",
      "1.0 -1.0\n",
      "RUN_2 Temperature [C]\n",
      "RUN_4 Temperature [C]\n",
      "RUN_1 Temperature [C]\n",
      "RUN_3 Temperature [C]\n",
      "6.65074867298968 -6.649386398890904\n",
      "RUN_0 Liquid X-Velocity [m_per_y]\n",
      "7.213143735226594 -4.985348762367756\n",
      "RUN_0 Liquid Y-Velocity [m_per_y]\n",
      "83.53336841228503 -7.012547789629725\n",
      "RUN_0 Liquid Z-Velocity [m_per_y]\n",
      "812254.1702669684 174851.14481186643\n",
      "RUN_0 Liquid_Pressure [Pa]\n",
      "3 1\n",
      "RUN_0 Material_ID\n",
      "13.671815063873437 10.596405903060218\n",
      "RUN_0 Temperature [C]\n"
     ]
    }
   ],
   "source": [
    "## data normalizing:\n",
    "# prep data: normalize all input values (vel/pressure) and all output values (temp/vel?), each to its own min,max, std\n",
    "\n",
    "# scaler-function for a row(=index) in the dataset\n",
    "def get_max_min(array):\n",
    "    return np.max(array), np.min(array)\n",
    "\n",
    "def scale_data_row(data, column, desired_max = 1, desired_min = -1):\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    for row in data.index:\n",
    "        max_temp, min_temp = get_max_min(data.at[row, column])\n",
    "        max_list.append(max_temp)\n",
    "        min_list.append(min_temp)\n",
    "    \n",
    "    max_data = np.max(max_list)\n",
    "    min_data = np.min(min_list)    \n",
    "    for row in data.index:\n",
    "        data_std = (data.at[row,column] - min_data) / (max_data - min_data)\n",
    "        data.at[row, column] = data_std * (desired_max - desired_min) + desired_min\n",
    "        print(row, column)\n",
    "        #print(np.max(data.at[row, column]), np.min(data.at[row, column]))\n",
    "\n",
    "    return data\n",
    "#test = [[[1,2], [1,2]], [[3,3], [3,3]], [[1,2], [1,2]], [[-10,10], [4,4]]]\n",
    "#test_res = scale_data_row(test)\n",
    "#print(np.std(test_res))\n",
    "\n",
    "def scale_data(data):\n",
    "    for column in data.columns:\n",
    "        data = scale_data_row(data, column)\n",
    "    return data\n",
    "\n",
    "df_train = scale_data(df_train)\n",
    "df_test = scale_data(df_test)\n",
    "\n",
    "# jut for manuel checking the values\n",
    "#for column in df_train.columns:\n",
    "#    for row in df_train.index:\n",
    "#        print(column, row, np.min(df_train.at[row, column]), np.max(df_train.at[row, column]), np.std(df_train.at[row, column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into input and labels\n",
    "df_train_in = pd.DataFrame(df_train[\"Liquid_Pressure [Pa]\"])\n",
    "df_train_labels = pd.DataFrame(df_train[[\"Material_ID\",\"Temperature [C]\"]])\n",
    "df_test_in = pd.DataFrame(df_test[\"Liquid_Pressure [Pa]\"])\n",
    "df_test_labels = pd.DataFrame(df_test[[\"Material_ID\",\"Temperature [C]\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "#shuffle_buffer_size = 10\n",
    "#\n",
    "### seperate dataset into input data and corresponding output labels\n",
    "#data_in = {}\n",
    "#data_labels = {}\n",
    "#\n",
    "#data_in['Liquid_Pressure [Pa]'] = data['Liquid_Pressure [Pa]'] #maybe for help in beginning: 'Liquid X-Velocity [m_per_y]', 'Liquid Y-Velocity [m_per_y]', 'Liquid Z-Velocity [m_per_y]']\n",
    "#data_in['Material_ID'] = data['Material_ID']\n",
    "#data_labels['Temperature [C]'] = data['Temperature [C]']\n",
    "#\n",
    "## create dataset for tf, slice along first dimension: datapoint\n",
    "## one element := one datapoint (RUN_XYZ); each datapoint is a dictionnary with [0]: input keys=\"physical properties\" (p,pos_hp), values =(x,y,z)-component, [1]:  output keys=\"physical properties\" (T), values =(x,y,z)-component\n",
    "#tf_data = tf.data.Dataset.from_tensor_slices((data_in, data_labels))\n",
    "##print(tf_data.element_spec)\n",
    "#print([elem[0]['Liquid_Pressure [Pa]'][0,0,0].numpy() for elem in tf_data])\n",
    "#\n",
    "## shuffle the data\n",
    "#tf_data = tf_data.shuffle(shuffle_buffer_size)\n",
    "#print([elem[0]['Liquid_Pressure [Pa]'][0,0,0].numpy() for elem in tf_data])\n",
    "#\n",
    "##np.random.seed(seed=1) # TODO just for programming and varification purposes at this point, delete later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "#train_size_percent = 0.75\n",
    "#test_size_percent = 0.25\n",
    "#\n",
    "#np.random.seed(seed=1) # TODO just for programming and varification purposes at this point, delete later!\n",
    "#\n",
    "#def split_dataset(train_size_percent, test_size_percent, dataset_pressure, dataset_temperature, dataset_vel_x, dataset_vel_y, dataset_vel_z):\n",
    "#    # calculating parameters from settings\n",
    "#    dataset_size = len(dataset_pressure)\n",
    "#    train_size = int(dataset_size * train_size_percent)\n",
    "#    test_size = int(dataset_size * test_size_percent)\n",
    "#\n",
    "#    # create random split of data\n",
    "#    indices = np.arange(0,100)\n",
    "#    np.random.shuffle(indices)\n",
    "#    indices_train = indices[:train_size]\n",
    "#    indices_test = indices[train_size:train_size+test_size]\n",
    "#\n",
    "#    # split dataset into training and validation data \n",
    "#    data_train_pressure = []\n",
    "#    data_train_temperature = []\n",
    "#    data_train_vel_x = []\n",
    "#    data_train_vel_y = []\n",
    "#    data_train_vel_z = []\n",
    "#\n",
    "#    data_test_pressure = []\n",
    "#    data_test_temperature = []\n",
    "#    data_test_vel_x = []\n",
    "#    data_test_vel_y = []\n",
    "#    data_test_vel_z = []\n",
    "#\n",
    "#    for index in indices_train:\n",
    "#        data_train_pressure.append(dataset_pressure[index])\n",
    "#        data_train_temperature.append(dataset_temperature[index])\n",
    "#        data_train_vel_x.append(dataset_vel_x[index])\n",
    "#        data_train_vel_y.append(dataset_vel_y[index])\n",
    "#        data_train_vel_z.append(dataset_vel_z[index])\n",
    "#\n",
    "#    for index in indices_test:\n",
    "#        data_test_pressure.append(dataset_pressure[index])\n",
    "#        data_test_temperature.append(dataset_temperature[index])\n",
    "#        data_test_vel_x.append(dataset_vel_x[index])\n",
    "#        data_test_vel_y.append(dataset_vel_y[index])\n",
    "#        data_test_vel_z.append(dataset_vel_z[index])\n",
    "#\n",
    "#    data_train_pressure = np.array(data_train_pressure)\n",
    "#    data_train_temperature = np.array(data_train_temperature)\n",
    "#    data_train_vel_x = np.array(data_train_vel_x)\n",
    "#    data_train_vel_y = np.array(data_train_vel_y)\n",
    "#    data_train_vel_z = np.array(data_train_vel_z)\n",
    "#    data_test_pressure = np.array(data_test_pressure)\n",
    "#    data_test_temperature = np.array(data_test_temperature)\n",
    "#    data_test_vel_x = np.array(data_test_vel_x)\n",
    "#    data_test_vel_y = np.array(data_test_vel_y)\n",
    "#    data_test_vel_z = np.array(data_test_vel_z)\n",
    "#\n",
    "#    return data_train_pressure, data_train_temperature, data_train_vel_x, data_train_vel_y, data_train_vel_z, data_test_pressure, data_test_temperature, data_test_vel_x, data_test_vel_y, data_test_vel_z\n",
    "\n",
    "#data_train_pressure, data_train_temperature, data_train_vel_x, data_train_vel_y, data_train_vel_z, data_test_pressure, data_test_temperature, data_test_vel_x, data_test_vel_y, data_test_vel_z = split_dataset(train_size_percent, test_size_percent, dataset_pressure_init, dataset_temperature, dataset_vel_x, dataset_vel_y, dataset_vel_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FCN model definition**\n",
    "\n",
    "adapted from https://github.com/himanshurawlani/fully_convolutional_network.git\n",
    "made it 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 3#(3, 3, 3, 1)\n",
    "def FCN_model(input_shape=(20,200,16,1), len_classes=5, dropout_rate=0.2, kernel=3):\n",
    "\n",
    "    input = Input(shape=input_shape) #2D: (None, None, 3))\n",
    "    x = Conv3D(filters=32, kernel_size=kernel, strides=1,padding=\"same\")(input) \n",
    "    #  filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv3D(filters=64, kernel_size=kernel, strides=1,padding= \"same\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv3D(filters=128, kernel_size=kernel, strides=2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv3D(filters=256, kernel_size=kernel, strides=2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "    # x = Conv3D(filters=512, kernel_size=kernel, strides=2)(x)\n",
    "    # x = Dropout(dropout_rate)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Activation('relu')(x)\n",
    "\n",
    "    # Uncomment the below line if you're using dense layers\n",
    "    # x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.Dense(units=64)(x)\n",
    "    # x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = Conv3D(filters=64, kernel_size=1, strides=1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.Dense(units=len_classes)(x)\n",
    "    # predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = Conv3D(filters=len_classes, kernel_size=1, strides=1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = GlobalMaxPooling3D()(x) #TODO I PERSONALLY EXCLUDED - BACK IN?\n",
    "    predictions = Activation('softmax')(x)\n",
    "\n",
    "    model = tensorflow.keras.Model(inputs=input, outputs=predictions)\n",
    "    \n",
    "    print(model.summary())\n",
    "    print(f'Total number of layers: {len(model.layers)}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 20, 200, 16, 1)]  0         \n",
      "                                                                 \n",
      " conv3d_37 (Conv3D)          (None, 20, 200, 16, 32)   896       \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 20, 200, 16, 32)   0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 20, 200, 16, 32)  128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 20, 200, 16, 32)   0         \n",
      "                                                                 \n",
      " conv3d_38 (Conv3D)          (None, 20, 200, 16, 64)   55360     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 20, 200, 16, 64)   0         \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 20, 200, 16, 64)  256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 20, 200, 16, 64)   0         \n",
      "                                                                 \n",
      " conv3d_39 (Conv3D)          (None, 9, 99, 7, 128)     221312    \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 9, 99, 7, 128)     0         \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 9, 99, 7, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 9, 99, 7, 128)     0         \n",
      "                                                                 \n",
      " conv3d_40 (Conv3D)          (None, 4, 49, 3, 256)     884992    \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 4, 49, 3, 256)     0         \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 4, 49, 3, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 4, 49, 3, 256)     0         \n",
      "                                                                 \n",
      " conv3d_41 (Conv3D)          (None, 4, 49, 3, 64)      16448     \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 4, 49, 3, 64)      0         \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 4, 49, 3, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 4, 49, 3, 64)      0         \n",
      "                                                                 \n",
      " conv3d_42 (Conv3D)          (None, 4, 49, 3, 5)       325       \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 4, 49, 3, 5)       0         \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 4, 49, 3, 5)      20        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 4, 49, 3, 5)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,181,529\n",
      "Trainable params: 1,180,431\n",
      "Non-trainable params: 1,098\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total number of layers: 25\n"
     ]
    }
   ],
   "source": [
    "model = FCN_model(input_shape=(20, 200, 16, 1), len_classes=5, dropout_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55b09931d414103c8b6886af0a1edcfe2c130d5f4db2734a847754e1dbe9ad78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('poetry-test-k7vgOhCE-py3.8': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
