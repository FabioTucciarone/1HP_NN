{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "import learn_process as lp\n",
    "from networks.unet_leiterrl import TurbNetG, UNet\n",
    "from networks.dummy_network import DummyNet\n",
    "import visualization.visualize_data as vis\n",
    "import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # model choice \n",
    "# model_choice = sys.argv[1]\n",
    "# if model_choice == \"unet\":\n",
    "#     model = unet_model\n",
    "# elif model_choice == \"fc\":\n",
    "#     model = fc_model\n",
    "# else:\n",
    "#     print(\"model choice not recognized\")\n",
    "#     sys.exit()\n",
    "\n",
    "\n",
    "# TODO overfit until not possible anymore (dummynet, then unet)\n",
    "# therefor: try to exclude connections from unet until it overfits properly (loss=0)\n",
    "# TODO go over input properties (delete some, some from other simulation with no hps?)\n",
    "# TODO: add 3D data\n",
    "# TODO : data augmentation, \n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# print(dataloaders_2D[\"train\"].dataset[0]['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(inputs, model_name, n_epochs, debugging=False, dataset_name=\"groundtruth_hps_no_hps/groundtruth_hps_overfit_10\", reduce_to_2D_wrong=False):\n",
    "    # set debugging to true if you dont wont a run folder (with loss history etc and a picture of the result)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.WARNING)  # level: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "\n",
    "    # parameters of model and training\n",
    "    loss_fn = MSELoss()\n",
    "    lr=0.0004 #0.0004\n",
    "\n",
    "    # model alternatives\n",
    "    #model = TurbNetG(channelExponent=4, in_channels=4, out_channels=2)\n",
    "    # TODO too many in channels for unet?\n",
    "    datasets_2D, dataloaders_2D = lp.init_data(reduce_to_2D=True, reduce_to_2D_wrong=reduce_to_2D_wrong, overfit=True, dataset_name=dataset_name, inputs=inputs) # init data\n",
    "    \n",
    "    if model_name == \"unet\":\n",
    "        unet_model = UNet(in_channels=len(inputs)+1, out_channels=1).float()\n",
    "        model = unet_model  # current parameter choice\n",
    "    elif model_name == \"dummy\":\n",
    "        dummy_model = DummyNet(in_channels=len(inputs)+1, out_channels=1).float()\n",
    "        model = dummy_model\n",
    "\n",
    "    #print(dataloaders_2D[\"train\"].dataset[0]['x'].shape)\n",
    "    \n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    name_folder=now+\"_\"+model_name+\"_overfit\"+dataset_name[-2:]+\"_\"+\"in_\"+inputs+\"_out_T_\"+str(n_epochs)+\"epochs\"\n",
    "    #summary(model, (len(inputs)+1, 128, 16))\n",
    "    \n",
    "    loss_hist = lp.train_model(model, dataloaders_2D, loss_fn, n_epochs, lr, name_folder=name_folder, debugging=debugging)                   # train model\n",
    "    error = None\n",
    "    if not debugging:\n",
    "        error = vis.plot_exemplary_learned_result(model, dataloaders_2D, name_pic=name_folder+\"/plot_y_\"+name_folder)    # visualize results, pic in folder runs/current model\n",
    "\n",
    "    return model, error, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 10000 #000 #7000   # 60000\n",
    "# inputs=\"p\"\n",
    "# model_name=\"unet\"\n",
    "# #try stride=1 in innerst loop\n",
    "# # try more layers? (depth) to capture the details?\n",
    "# \n",
    "# model, _, _ = do_it(inputs, model_name, n_epochs, dataset_name=\"groundtruth_hps_no_hps/groundtruth_hps_overfit_01\") #,reduce_to_2D_wrong=True) \n",
    "\n",
    "# TODO check out normalization - everywhere excluded?? for unet since unet does batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dummy with input variables: p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   2%|‚ñè         | 206/10000 [01:01<52:49,  3.09it/s, loss: 0.2243]  "
     ]
    }
   ],
   "source": [
    "# # TODO RUN ALL MODELS THIS EVENING\n",
    "\n",
    "#test unet with 10 datapoints\n",
    "n_epochs = 10000     #60000\n",
    "model_names = [\"dummy\", \"unet\"]\n",
    "inputs_combos = [\"p\", \"xy\"] #\"yz\", \"yp\", \"zp\", \"yzp\", \"yzt\", \"pt\", \"yztp\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    for inputs in inputs_combos:\n",
    "        print(f\"Model: {model_name} with input variables: {inputs}\")\n",
    "        do_it(inputs, model_name, n_epochs, dataset_name=\"groundtruth_hps_no_hps/groundtruth_hps_overfit_10\")\n",
    "\n",
    "# test xy instead of yz 2D unet\n",
    "for model_name in model_names:\n",
    "    for inputs in inputs_combos:\n",
    "        print(f\"Model: {model_name} with input variables: {inputs}\")\n",
    "        do_it(inputs, model_name, n_epochs, reduce_to_2D_wrong=True, dataset_name=\"groundtruth_hps_no_hps/groundtruth_hps_overfit_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
