{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import GWF_HP_Dataset\n",
    "from data.dataloader import DataLoader\n",
    "from data.transforms import NormalizeTransform, ComposeTransform, ReduceTo2DTransform, PowerOfTwoTransform\n",
    "from visualization.visualize_data import plot_datapoint\n",
    "from data.utils_save import save_pickle\n",
    "from networks.unet_leiterrl import TurbNetG, UNet, weights_init\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "COLOR = 'white'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO not necessary to cut of edges - exponential behaviour at edges ?! fix it otherwise!\n",
    "#transforms = ComposeTransform([NormalizeTransform(), CutOffEdgesTransform()])\n",
    "\n",
    "# DATASET = GWF_HP_Dataset(dataset_name =\"dataset_HDF5_testtest\", transform = NormalizeTransform(), \n",
    "#                  input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\", \n",
    "#                  \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "#                  output_vars=[\"Liquid_Pressure [Pa]\", \"Temperature [C]\"])\n",
    "# DATALOADER = DataLoader(DATASET, batch_size=2, shuffle=True, drop_last=False)\n",
    "# \n",
    "# print(f\"Dataset size: {len(DATASET)}\")\n",
    "# print(f\"Dataloader size: {len(DATALOADER)}\")\n",
    "# \n",
    "# save_pickle({\"dataset\": DATASET, \"dataloader\" : DATALOADER}, \"dataset_HDF5_testtest_and_dataloader.p\")\n",
    "# \n",
    "# DATASET[0]['x'][0,:,:,:].shape\n",
    "# plot_datapoint(DATASET, run_id=1, view=\"side_hp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split dataset into train, val, test\n",
    "# datasets = {}\n",
    "# for mode in ['train', 'val', 'test']:\n",
    "#     temp_dataset = GWF_HP_Dataset(\n",
    "#         dataset_name =\"dataset_HDF5_testtest\", transform = NormalizeTransform(), \n",
    "#         input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\", \n",
    "#         \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "#         output_vars=[\"Liquid_Pressure [Pa]\", \"Temperature [C]\"],\n",
    "#         mode=mode, split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "#     )\n",
    "#     datasets[mode] = temp_dataset\n",
    "# \n",
    "# # Create a dataloader for each split.\n",
    "# dataloaders = {}\n",
    "# for mode in ['train', 'val', 'test']:\n",
    "#     temp_dataloader = DataLoader(\n",
    "#         dataset=datasets[mode],\n",
    "#         batch_size=2,\n",
    "#         shuffle=True,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "#     dataloaders[mode] = temp_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datasets[\"train\"].runs)\n",
    "# print(datasets[\"train\"][0].keys())\n",
    "# print(len(dataloaders[\"train\"]))\n",
    "# \n",
    "# for batch in dataloaders[\"train\"]:\n",
    "#     print(batch['x'].shape)\n",
    "#     print(batch['y'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test visualization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_datapoint(datasets[\"train\"], run_id=0, view=\"side_hp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplest test NN (linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test TurbNetG (from Rapha, from somebody else) 2D testcase on my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n",
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n",
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train, val, test\n",
    "\n",
    "def init_data(reduce_to_2D = True):\n",
    "    \n",
    "    datasets = {}\n",
    "    if reduce_to_2D:\n",
    "        transforms = ComposeTransform([NormalizeTransform(), PowerOfTwoTransform(), ReduceTo2DTransform()])\n",
    "    else:\n",
    "        transforms = ComposeTransform([NormalizeTransform()]) # PowerOfTwoTransform()\n",
    "\n",
    "    for mode in ['train', 'val', 'test']:\n",
    "        temp_dataset = GWF_HP_Dataset(\n",
    "            dataset_name =\"dataset_HDF5_testtest\", transform = transforms,\n",
    "            input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\", \n",
    "            \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "            output_vars=[\"Liquid_Pressure [Pa]\", \"Temperature [C]\"],\n",
    "            mode=mode, split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "        )\n",
    "        datasets[mode] = temp_dataset\n",
    "\n",
    "    # Create a dataloader for each split.\n",
    "    dataloaders = {}\n",
    "    for mode in ['train', 'val', 'test']:\n",
    "        temp_dataloader = DataLoader(\n",
    "            dataset=datasets[mode],\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        dataloaders[mode] = temp_dataloader\n",
    "\n",
    "    # Assert if data is not 2D\n",
    "    def assertion_error_2d(datasets):\n",
    "        for dataset in datasets[\"train\"]:\n",
    "            shape_data = len(dataset['x'].shape)\n",
    "            break\n",
    "        assert shape_data == 3, \"Data is not 2D\"\n",
    "\n",
    "    assertion_error_2d(datasets)\n",
    "\n",
    "    return datasets, dataloaders\n",
    "\n",
    "datasets_2D, dataloaders_2D = init_data(reduce_to_2D=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss: 1.7626]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = MSELoss()\n",
    "n_epochs = 100 #60000\n",
    "\n",
    "#model = TurbNetG(channelExponent=4, in_channels=5, out_channels=2)\n",
    "model = UNet(in_channels=5, out_channels=2)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001) #0.0004\n",
    "# model.to(device)\n",
    "model.apply(weights_init)\n",
    "loss_hist = []\n",
    "epochs = tqdm(range(n_epochs), desc = \"epochs\")\n",
    "for epoch in epochs:\n",
    "    for batch_idx, data_point in enumerate(dataloaders_2D[\"train\"]):\n",
    "        # TODO in welchem Format das input und target angegeben werden muss\n",
    "        x = data_point[\"x\"]\n",
    "        y = data_point[\"y\"]\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_out = model(x)\n",
    "        mse_loss = loss_fn(y_out, y)\n",
    "        loss = mse_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochs.set_postfix_str(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        loss_hist.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
