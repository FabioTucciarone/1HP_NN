{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import GWF_HP_Dataset\n",
    "from data.dataloader import DataLoader\n",
    "from data.transforms import NormalizeTransform, ComposeTransform, ReduceTo2DTransform, PowerOfTwoTransform, ToTensorTransform\n",
    "from visualization.visualize_data import plot_datapoint\n",
    "from data.utils_save import save_pickle\n",
    "from networks.unet_leiterrl import TurbNetG, UNet, weights_init\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "COLOR = 'white'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO not necessary to cut of edges - exponential behaviour at edges ?! fix it otherwise!\n",
    "#transforms = ComposeTransform([NormalizeTransform(), CutOffEdgesTransform()])\n",
    "\n",
    "# DATASET = GWF_HP_Dataset(dataset_name =\"dataset_HDF5_testtest\", transform = NormalizeTransform(), \n",
    "#                  input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\", \n",
    "#                  \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "#                  output_vars=[\"Liquid_Pressure [Pa]\", \"Temperature [C]\"])\n",
    "# DATALOADER = DataLoader(DATASET, batch_size=2, shuffle=True, drop_last=False)\n",
    "# \n",
    "# print(f\"Dataset size: {len(DATASET)}\")\n",
    "# print(f\"Dataloader size: {len(DATALOADER)}\")\n",
    "# \n",
    "# save_pickle({\"dataset\": DATASET, \"dataloader\" : DATALOADER}, \"dataset_HDF5_testtest_and_dataloader.p\")\n",
    "# \n",
    "# DATASET[0]['x'][0,:,:,:].shape\n",
    "# plot_datapoint(DATASET, run_id=1, view=\"side_hp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split dataset into train, val, test\n",
    "# datasets = {}\n",
    "# for mode in ['train', 'val', 'test']:\n",
    "#     temp_dataset = GWF_HP_Dataset(\n",
    "#         dataset_name =\"dataset_HDF5_testtest\", transform = NormalizeTransform(), \n",
    "#         input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\", \n",
    "#         \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "#         output_vars=[\"Liquid_Pressure [Pa]\", \"Temperature [C]\"],\n",
    "#         mode=mode, split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "#     )\n",
    "#     datasets[mode] = temp_dataset\n",
    "# \n",
    "# # Create a dataloader for each split.\n",
    "# dataloaders = {}\n",
    "# for mode in ['train', 'val', 'test']:\n",
    "#     temp_dataloader = DataLoader(\n",
    "#         dataset=datasets[mode],\n",
    "#         batch_size=2,\n",
    "#         shuffle=True,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "#     dataloaders[mode] = temp_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datasets[\"train\"].runs)\n",
    "# print(datasets[\"train\"][0].keys())\n",
    "# print(len(dataloaders[\"train\"]))\n",
    "# \n",
    "# for batch in dataloaders[\"train\"]:\n",
    "#     print(batch['x'].shape)\n",
    "#     print(batch['y'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplest test NN (linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test TurbNetG (from Rapha, from somebody else) 2D testcase on my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, val, test\n",
    "def init_data(reduce_to_2D = True, overfit = False, normalize=True):\n",
    "    \n",
    "    datasets = {}\n",
    "    transforms_list = [ToTensorTransform(), PowerOfTwoTransform(oriented=\"left\")]\n",
    "    if normalize:\n",
    "        transforms_list.append(NormalizeTransform())\n",
    "    if reduce_to_2D:\n",
    "        transforms_list = transforms_list.append(ReduceTo2DTransform())\n",
    "\n",
    "    transforms = ComposeTransform(transforms_list)\n",
    "    split = {'train': 0.6, 'val': 0.2, 'test': 0.2} if not overfit else {'train': 0.2, 'val': 0.2, 'test': 0.6}\n",
    "    \n",
    "    for mode in ['train', 'val', 'test']:\n",
    "        temp_dataset = GWF_HP_Dataset(\n",
    "            dataset_name =\"dataset_HDF5_testtest\", transform = transforms,\n",
    "            input_vars=[\"Liquid Y-Velocity [m_per_y]\", \"Liquid Z-Velocity [m_per_y]\",  #\"Liquid X-Velocity [m_per_y]\",\n",
    "            \"Liquid_Pressure [Pa]\", \"Material_ID\", \"Temperature [C]\"],\n",
    "            output_vars=[\"Temperature [C]\"], #. \"Liquid_Pressure [Pa]\"\n",
    "            mode=mode, split=split\n",
    "        )\n",
    "        datasets[mode] = temp_dataset\n",
    "\n",
    "\n",
    "    # Create a dataloader for each split.\n",
    "    dataloaders = {}\n",
    "    for mode in ['train', 'val', 'test']:\n",
    "        temp_dataloader = DataLoader(\n",
    "            dataset=datasets[mode],\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        dataloaders[mode] = temp_dataloader\n",
    "\n",
    "    # # Assert if data is not 2D\n",
    "    # def assertion_error_2d(datasets):\n",
    "    #     for dataset in datasets[\"train\"]:\n",
    "    #         shape_data = len(dataset['x'].shape)\n",
    "    #         break\n",
    "    #     assert shape_data == 3, \"Data is not 2D\"\n",
    "    # \n",
    "    # assertion_error_2d(datasets)\n",
    "\n",
    "    return datasets, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n",
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n",
      "Directory of currently used dataset is: /home/pelzerja/Development/simulation_groundtruth_pflotran/Phd_simulation_groundtruth/approach2_dataset_generation_simplified/dataset_HDF5_testtest\n",
      "<data.dataset.GWF_HP_Dataset object at 0x7fc61c45bdc0>\n",
      "<data.dataset.GWF_HP_Dataset object at 0x7fc61c45bb20>\n",
      "<data.dataset.GWF_HP_Dataset object at 0x7fc61c45b3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=10'>11</a>\u001b[0m datasets_2D, dataloaders_2D \u001b[39m=\u001b[39m init_data(reduce_to_2D\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, overfit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=12'>13</a>\u001b[0m \u001b[39m# running the training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=13'>14</a>\u001b[0m train_model(unet_model, dataloaders_2D, loss_fn, n_epochs, lr)\n",
      "\u001b[1;32m/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, loss_fn, n_epochs, lr)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=6'>7</a>\u001b[0m epochs \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(n_epochs), desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m epochs:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, data_point \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=9'>10</a>\u001b[0m         \u001b[39m# TODO in welchem Format das input und target angegeben werden muss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=10'>11</a>\u001b[0m         x \u001b[39m=\u001b[39m data_point[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pelzerja/Development/3D_FCN/test_visualise_fcnn_i2dl.ipynb#ch0000038?line=11'>12</a>\u001b[0m         y \u001b[39m=\u001b[39m data_point[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Development/3D_FCN/data/dataloader.py:82\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Development/3D_FCN/data/dataset.py:195\u001b[0m, in \u001b[0;36mGWF_HP_Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    193\u001b[0m data_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    194\u001b[0m index_material_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_input_properties()\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39mMaterial_ID\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m data_dict[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m], data_dict[\u001b[39m\"\u001b[39m\u001b[39mx_mean\u001b[39m\u001b[39m\"\u001b[39m], data_dict[\u001b[39m\"\u001b[39m\u001b[39mx_std\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data_as_numpy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_paths[index], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_vars), index_material_id\u001b[39m=\u001b[39;49mindex_material_id)\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_material_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m data_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m], data_dict[\u001b[39m\"\u001b[39m\u001b[39my_mean\u001b[39m\u001b[39m\"\u001b[39m], data_dict[\u001b[39m\"\u001b[39m\u001b[39my_std\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data_as_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_paths[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_vars))\n",
      "File \u001b[0;32m~/Development/3D_FCN/data/transforms.py:151\u001b[0m, in \u001b[0;36mComposeTransform.__call__\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m    152\u001b[0m         data \u001b[39m=\u001b[39m transform(data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# parameters of model and training\n",
    "loss_fn = MSELoss()\n",
    "n_epochs = 1000 #60000\n",
    "lr=0.0004 #0.0004\n",
    "\n",
    "#model = TurbNetG(channelExponent=4, in_channels=4, out_channels=2)\n",
    "unet_model = UNet(in_channels=5, out_channels=1)\n",
    "fc_model = nn.Sequential(nn.Flatten(), nn.Linear(5*128*16, 1*128*16), nn.Unflatten(dim=1,unflattened_size=[1,128,16]))\n",
    "# model.to(device)\n",
    "\n",
    "datasets_2D, dataloaders_2D = init_data(reduce_to_2D=True, overfit=True)\n",
    "\n",
    "# running the training\n",
    "train_model(unet_model, dataloaders_2D, loss_fn, n_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, loss_fn, n_epochs, lr):\n",
    "    optimizer = Adam(model.parameters(), lr=lr) \n",
    "    writer = SummaryWriter()\n",
    "    loss_hist = []\n",
    "    \n",
    "    model.apply(weights_init)\n",
    "    epochs = tqdm(range(n_epochs), desc = \"epochs\")\n",
    "    for epoch in epochs:\n",
    "        for batch_idx, data_point in enumerate(dataloaders[\"train\"]):\n",
    "            # TODO in welchem Format das input und target angegeben werden muss\n",
    "            x = data_point[\"x\"]\n",
    "            y = data_point[\"y\"]\n",
    "\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_out = model(x) # dimensions: batch-datapoint_id, channel, x, y\n",
    "            mse_loss = loss_fn(y_out, y)\n",
    "            loss = mse_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochs.set_postfix_str(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "            loss_hist.append(loss.item())\n",
    "            writer.add_scalar(\"loss\", loss.item(), epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"x_0\", x[0,0,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"x_1\", x[0,1,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"x_2\", x[0,2,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"x_3\", x[0,3,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"x_4\", x[0,4,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # writer.add_image(\"y_0\", y[0,0,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            # #writer.add_image(\"y_1\", y[0,1,:,:], dataformats=\"WH\")\n",
    "            # writer.add_image(\"y_out_0\", y_out[0,0,:,:], dataformats=\"WH\", global_step=epoch*len(dataloaders[\"train\"])+batch_idx)\n",
    "            #writer.add_image(\"y_out_1\", y_out[0,1,:,:], dataformats=\"WH\")\n",
    "\n",
    "    #writer.add_graph(model, x)\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([item for item in dataloaders_2D[\"train\"]])\n",
    "print(dataloaders_2D[\"train\"].dataset[0]['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.visualize_data import aligned_colorbar\n",
    "\n",
    "def slice_y(y, property):\n",
    "    property = 0 if property == \"temperature\" else property #1\n",
    "    return y.detach().numpy()[0,property,:,:]\n",
    "\n",
    "def plot_y(data):\n",
    "    n_subplots = len(data)\n",
    "    fig, axes = plt.subplots(n_subplots,1,sharex=True,figsize=(20,3*(n_subplots)))\n",
    "    plt.title(\"Exemplary Comparison Input Output\")\n",
    "    \n",
    "    for index, data_point in enumerate(data):\n",
    "        plt.sca(axes[index])\n",
    "        plt.imshow(slice_y(data_point[\"data\"], data_point[\"property\"]).T)\n",
    "        aligned_colorbar(label=data_point[\"property\"])\n",
    "\n",
    "    \n",
    "\n",
    "for data_test in dataloaders_2D[\"train\"]:\n",
    "    y_true_test = data_test[\"y\"]\n",
    "    y_out_test = model(data_test[\"x\"])\n",
    "    x = data_test[\"x\"]\n",
    "    break\n",
    "\n",
    "def make_dict(data, property):\n",
    "    dict = {\"data\" : data, \"property\" : property}\n",
    "    return dict\n",
    "\n",
    "list_to_plot = [\n",
    "    make_dict(y_true_test, \"temperature\"),\n",
    "    make_dict(y_out_test, \"temperature\"),\n",
    "    make_dict(x, 0),\n",
    "    make_dict(x, 1),\n",
    "    make_dict(x, 2),\n",
    "    make_dict(x, 3),\n",
    "    make_dict(x, 4)\n",
    "]\n",
    "\n",
    "plot_y(list_to_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_point in dataloaders_2D[\"test\"]:\n",
    "#     x_test = test_point[\"x\"]\n",
    "#     y_out_test = model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test visualization and normalization and reverse of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from visualization.visualize_data import plot_data_inner\n",
    "# \n",
    "# datasets, dataloaders = init_data(reduce_to_2D=False, overfit=True)\n",
    "# tensor_dict = datasets[\"train\"][0]\n",
    "# tensor_reversed_norm = datasets[\"train\"].reverse_transform(0, tensor_dict[\"x_mean\"], tensor_dict[\"x_std\"], tensor_dict[\"y_mean\"], tensor_dict[\"y_std\"])\n",
    "# \n",
    "# prop_in = datasets[\"train\"].get_input_properties()\n",
    "# prop_out = datasets[\"train\"].get_output_properties()\n",
    "# \n",
    "# plot_data_inner(tensor_reversed_norm, prop_in, prop_out, view=\"side_hp\", oriented=\"left\")\n",
    "# \n",
    "# plot_datapoint(datasets[\"train\"], run_id=0, view=\"side_hp\", oriented=\"left\")\n",
    "# # plot_datapoint(datasets[\"train\"], run_id=0, view=\"top_hp\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
